{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# <font color=\"blue\">Workshop : Hands-on Data PreProcessing using Python</font>\n",
    "\n",
    "- by Shivam Bansal | Feb 20, 2019 | NUS BAC\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Contents \n",
    "\n",
    "1. Data Pre-Processing     \n",
    "2. PreProcessing - Numerical Data  \n",
    "3. PreProcessing - Text Data  \n",
    "\n",
    "\n",
    "<br>\n",
    "<hr> \n",
    "\n",
    "### <font color=\"blue\">1. What is Data PreProcessing ?</font> \n",
    "\n",
    "#### Life Cycle of a Data Science Project \n",
    "\n",
    "A typical data science project lifecycle consists of 7 main steps : \n",
    "\n",
    "1. Business Use Case Understanding   \n",
    "2. Relevant Data Collection   \n",
    "3. Data PreProcessing   \n",
    "4. Exploration of Data   \n",
    "5. New Features Engineering    \n",
    "6. Modelling  - Descriptive or Predictive    \n",
    "7. Insights Extraction   \n",
    "\n",
    "<img src=\"http://sudeep.co/images/post_images/2018-02-09-Understanding-the-Data-Science-Lifecycle/chart.png\" width=400>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Effort and Time Spend \n",
    "\n",
    "<img src=\"https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fgilpress%2Ffiles%2F2016%2F03%2FTime-1200x511.jpg\">\n",
    "\n",
    "<br><br><br><hr>\n",
    "\n",
    "### <font color=\"blue\">What is Data Preprocessing ?</font> \n",
    "\n",
    "Real-world data is often incomplete, inconsistent, and/or lacking in certain behaviors or trends, and is likely to contain many errors.      \n",
    "\n",
    "<br>\n",
    "For Example: \n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src=\"https://i.imgur.com/qSDfmu5.png\" width=600>\n",
    "<br>\n",
    "\n",
    "Another Example:   \n",
    "\n",
    "<br>\n",
    "<img src=\"https://i.imgur.com/xZl4Wtt.png\" width=500>\n",
    "<br>\n",
    "\n",
    "Another Example:   \n",
    "\n",
    "<br>\n",
    "<img src=\"https://powerspreadsheets.com/wp-content/uploads/excel-date-format-dddd-mmmm-dd-yyyy.jpg\" width=500>\n",
    "<br>\n",
    "\n",
    "\n",
    "<b>Data preprocessing is a systematic process of cleaning a dataset so that a raw dataset is converted into a standardized one.</b>   \n",
    "\n",
    "**Why DataPreProcessing ?** \n",
    "- Quality of your inputs decide the quality of your output   \n",
    "-- Uncleaned Data can lead to a biased analysis or a biased model  \n",
    "-- Uncleaned Data can produce to noisy and wrong results  \n",
    "- Without preprocessing, the problem size increases  \n",
    "-- model's can become very complex   \n",
    "-- more training time  \n",
    "-- high chances of non-convergence  \n",
    "-- Without preprocessing, dimentionality of data space is very huge, more computation is required   \n",
    "- Better Interpretability of results and model outputs   \n",
    "- Can lead to improvement in accuracy  \n",
    "\n",
    "\n",
    "<br><hr> \n",
    "\n",
    "### <font color=\"blue\">2 : PreProcessing Numerical Data</font> \n",
    "\n",
    "Dataset : House Price Prediction\n",
    "Label : House Price  \n",
    "Features : Attributes of a House  \n",
    "\n",
    "First load the dataset in a pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e7479dcad82a89182580d71c93c8cc481fc2ae3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv(\"data/house_prices.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "23f7d3400dda80215fc8e2d2a14556b6b0004bab"
   },
   "source": [
    "Ideally, the first step is to understand the data and different variables. It involves: \n",
    "- identify the target variable and the independent variables  \n",
    "- identfiy the continuous and categorical variables  \n",
    "- identify the datatypes of different variables   \n",
    "\n",
    "### 2.1 Missing Values Treatment   \n",
    "\n",
    "1. Identify the columns with missing values  \n",
    "2. Individually handle the columns  \n",
    "    -- Drop the columns having large missing values  \n",
    "    -- Impute the continuous columns by stratified mean / median  \n",
    "    -- Impute the categorical columns by stratified mode  \n",
    "    -- Predict the missing values using regression models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "970b38ac2af5320dd41248e77f1ae10ab7914dfe"
   },
   "outputs": [],
   "source": [
    "## 1. identify the columns with missing values \n",
    "def get_missing_columns():\n",
    "    missing_df = data.isna().sum(axis=0).to_frame()\n",
    "    missing_df = missing_df.reset_index().rename(columns = {0:\"missing_count\"})\n",
    "    missing_df = missing_df.sort_values(\"missing_count\", ascending = False)\n",
    "    missing_df = missing_df[missing_df[\"missing_count\"] > 0]\n",
    "    return missing_df\n",
    "\n",
    "get_missing_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29c1573ff3d0e2885dce70b9218b302bda8e8c64"
   },
   "outputs": [],
   "source": [
    "## 1. Drop Columns \n",
    "drop_cols = [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\",  \"FireplaceQu\"]\n",
    "data = data.drop(drop_cols, axis = 1)\n",
    "get_missing_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "030752b7162c68a2d6ec201a8c41cbae1ff4d135"
   },
   "outputs": [],
   "source": [
    "## 2. Stratified Replacement\n",
    "data[\"LotFrontage\"]\n",
    "\n",
    "lookup = {}\n",
    "for i,row in data.groupby(\"Neighborhood\").agg({\"LotFrontage\" : \"median\"}).reset_index().iterrows():\n",
    "    key = row[\"Neighborhood\"]\n",
    "    value = row[\"LotFrontage\"]\n",
    "    lookup[key] = value  \n",
    "lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2efd5fbdc99ef15de935524c4cd37be19c46cf06"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "data[\"LotFrontage\"] = data.apply(lambda x : lookup[x[\"Neighborhood\"]] if pd.isnull(x[\"LotFrontage\"]) else x[\"LotFrontage\"]  , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "134cce0c9505cbb8b6ed71b10952879d10b9f718"
   },
   "outputs": [],
   "source": [
    "data[\"LotFrontage\"]\n",
    "get_missing_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef48110c2aab23deae65e415b76c5335c403ad64"
   },
   "outputs": [],
   "source": [
    "cols_to_fix = [c for c in data.columns if \"Garage\" in c]\n",
    "\n",
    "for c in cols_to_fix:\n",
    "    if data[c].dtype == \"object\":\n",
    "        data[c] = data[c].fillna(\"NA\")\n",
    "    else:\n",
    "        data[c] = data[c].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8aa3cf72e94950de7bc2261666f09acc32d675d2"
   },
   "outputs": [],
   "source": [
    "data[data[\"GarageType\"].isna()][cols_to_fix].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a42b9e77fccbcee66292c1fdf0c3717df0c6522d"
   },
   "outputs": [],
   "source": [
    "get_missing_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51fdc00caa2933f9c4a9a92c4e7bc437292030e0"
   },
   "outputs": [],
   "source": [
    "cols_to_fix += [c for c in data.columns if \"Bsmt\" in c]\n",
    "cols_to_fix += [c for c in data.columns if \"Mas\" in c]\n",
    "\n",
    "for c in cols_to_fix:\n",
    "    if data[c].dtype == \"object\":\n",
    "        data[c] = data[c].fillna(\"NA\")\n",
    "    else:\n",
    "        data[c] = data[c].fillna(0.0)\n",
    "\n",
    "get_missing_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20b7177636398cc30df6f6ae5609ee2058372bf8"
   },
   "outputs": [],
   "source": [
    "data['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])\n",
    "get_missing_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "032c9ac1ffd73bcb960d70cef4d953b0cc3bf374"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a7c62977c806cf638c02a568f584875267785226"
   },
   "source": [
    "### 2.2 Outliers Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10eb8df13b251ba85adbfa6c4de65339386882d7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = data['GrLivArea'], y = data['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2c94ecde446b540c3e7011031f6b8f3134b4279"
   },
   "outputs": [],
   "source": [
    "#Deleting outliers\n",
    "data = data.drop(data[(data['GrLivArea']>4000) & (data['SalePrice']<300000)].index)\n",
    "\n",
    "#Check the graphic again\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data['GrLivArea'], data['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63108425ba940fd04190da19be0c9f587f498ad5"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data['GarageArea'], data['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GarageArea', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7124ef589c5537f63e97a50f89e7fcf735840e51"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data['YearBuilt'], data['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('YearBuilt', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f0644022445c42ecd7513feef98e0afa081f55f"
   },
   "source": [
    "### 2.3 Removal of Least Informative Columns\n",
    "\n",
    "-- Almost Constant Columns (Less Variation)  \n",
    "-- Duplicate Columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "be0d73e26cbc4525a1ac1a219f3344915b1e54de"
   },
   "outputs": [],
   "source": [
    "var_df = data.var().to_frame().reset_index().rename(columns = {\"index\" : \"column\", 0:\"variance\"})\n",
    "var_df = var_df.sort_values(\"variance\", ascending = True)\n",
    "var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7e764738ebb2fd99e88768602dd65b396c8461c"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == \"object\":\n",
    "        data[col] = le.fit_transform(data[col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9166c29fb197132ffdc490881e3c250fd59c9566"
   },
   "outputs": [],
   "source": [
    "var_df = data.var().to_frame().reset_index().rename(columns = {\"index\" : \"column\", 0:\"variance\"})\n",
    "var_df = var_df.sort_values(\"variance\", ascending = True)\n",
    "var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Utilities', \"Street\", \"Id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb40723ad8c8d3f14f4f33951d4bc8190a981536"
   },
   "outputs": [],
   "source": [
    "## Duplicate Rows \n",
    "print (data.shape)\n",
    "data[\"a_duplicate_columns1\"] = data[\"MasVnrArea\"]\n",
    "data[\"a_duplicate_columns2\"] = data[\"MasVnrArea\"]\n",
    "data[\"a_duplicate_columns3\"] = data[\"MasVnrArea\"]\n",
    "data[\"a_duplicate_columns4\"] = data[\"MasVnrArea\"]\n",
    "data[\"a_duplicate_columns5\"] = data[\"MasVnrArea\"]\n",
    "data[\"a_duplicate_columns6\"] = data[\"MasVnrArea\"]\n",
    "print (data.shape)\n",
    "\n",
    "# data.drop_duplicates()\n",
    "data = data.T.drop_duplicates().T\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34e331d617df6244b6a256c8fb25faecf463b4ab"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b6f67cdf7f20e5d99a7f5e14ec309a6d70d6c82"
   },
   "outputs": [],
   "source": [
    "## Normalization \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "numerical_columns = [column for column in data.columns if data[column].dtype != \"object\"]\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "data[numerical_columns] = sc.fit_transform(data[numerical_columns].values)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cdb2b86d47d79362e078ac9ce2edeb6f7a4261e6"
   },
   "source": [
    "Recap  \n",
    "-- Missing Value Treatment  \n",
    "-- Outlier Treatment  \n",
    "-- Redundant Columns Removal  \n",
    "-- Normalization  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "\n",
    "## 3 : Handson PreProcessing of Text Data \n",
    "\n",
    "1. Stopwords Removal\n",
    "2. Special Characters Removal\n",
    "3. Text Normalization (LowerCasing, Lemmtization, Stemming)\n",
    "4. Regular Expressions based Removal  - hashtags, urls, mentions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0b6e4d92bc09a3c9f0acbb45bdaa7da82829d3c"
   },
   "outputs": [],
   "source": [
    "textdf = pd.read_csv(\"data/finance_tweets.csv\", error_bad_lines = False)\n",
    "textdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88e4faf59e808350604663709fd82d19ee1aa792"
   },
   "outputs": [],
   "source": [
    "import string \n",
    "punc = string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom = [\"@\", \"#\", \"www\", \"http\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f3e66407403b86f213b5acc1f781e07eadb6f9b0"
   },
   "outputs": [],
   "source": [
    "def clean_my_text(text):\n",
    "    \n",
    "    ## lower casing \n",
    "    text = text.lower()\n",
    "    # PLAYING, Playing, playing >>> playing \n",
    "    \n",
    "    ## Stopwords Removal\n",
    "    words = text.split()\n",
    "    text1 = \"\"\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            text1 += word \n",
    "            text1 += \" \"\n",
    "\n",
    "    ## unnecessary noise removal\n",
    "    words = text1.split()\n",
    "    text2 = \"\"\n",
    "    for word in words:\n",
    "        if any(word.startswith(x) for x in custom):\n",
    "            continue\n",
    "        text2 += word \n",
    "        text2 += \" \"\n",
    "    \n",
    "    ## Punctuations Removal\n",
    "    cleaned_text = \"\".join([char for char in text2 if char not in punc])\n",
    "    \n",
    "    ## Remove extra space\n",
    "    cleaned_text \n",
    "    return cleaned_text\n",
    "    \n",
    "clean_my_text(\"\"\"Today We are learning DATA PrEProcessing !!!. Hey??? We are learning at \n",
    "              Business - Analytics Center !!!. #datascience @Prof help us. Sign up @ https://msba.com . 12 23 405 2019 \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5d43af6e6d123511ef15b1974cae533d9553cc7"
   },
   "outputs": [],
   "source": [
    "textdf[\"cleaned\"] = textdf[\"text\"].apply(clean_my_text)\n",
    "textdf[[\"text\", \"cleaned\"]][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d073f720dcd1252358022539f24be90331d40b6b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
